import { Tabs } from 'nextra/components';
import OpenAiController from '../../downloaded-examples/openai/OpenAiController.mdx';
import OpenAiExample from '../../downloaded-examples/openai/OpenAiExample.mdx';
import LiveOpenAiExample from '../../live-examples/LiveOpenAiExample';

# OpenAI chat live example

<div className="doc-live-example">
    <LiveOpenAiExample />
</div>

View full code for this example on the [examples website](https://vovk-examples.vercel.app/openai).

`createChatCompletion` generator delegates the async iterable returned from `openai.chat.completions.create`
and the component utilises `using` keyword in order to close the stream safely in case of an interruption, iterating the messages using `await for` syntax.

```ts
import { type VovkRequest, post, prefix } from 'vovk';
import OpenAI from 'openai';

@prefix('openai')
export default class OpenAiController {
  private static openai = new OpenAI();

  @post('chat')
  static async *createChatCompletion(
    req: VovkRequest<{ messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] }>
  ) {
    const { messages } = await req.json();
    yield* await this.openai.chat.completions.create({
      messages,
      model: 'gpt-4o-mini',
      stream: true,
    });
  }
}
```

Usage on client side:

```ts
import { OpenAiRPC } from 'vovk-client';

// ...

// you can use "await using" suntax as well
using completion = await OpenAiRPC.createChatCompletion({
  body: { messages: [...messages, userMessage] },
});

for await (const chunk of completion) {
  // ...
}
```



