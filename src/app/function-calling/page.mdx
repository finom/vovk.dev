# Function Calling

Vovk.ts provides LLM function calling capabilities, turning route handlers into functions callable by AI. This enables more interactive user experiences, including interactions via a [text chat interface, a Realtime voice interface](/realtime-ui/ai), or [MCP](/mcp).

While many AI libraries and APIs focus on how to use function calling, Vovk.ts focuses on what should be callable.

This feature is implemented by a zero-dependency function, `createLLMTools`, imported from `"vovk"`. It produces a `tools` array that can be mapped to function-calling inputs for any LLM library. Each tool, converted from a controller or RPC handler, implements the `VovkLLMTool` interface with the following properties:

- `type`: the string literal `"function"`.
- `name`: the function name in the form `${moduleName}_${handlerName}` (module name + underscore + method name), or overridden by a custom `x-tool-name`.
- `description`: a description derived from OpenAPI metadata, concatenating the `summary` and `description` fields, or overridden by a custom `x-tool-description`. If neither is provided, `createLLMTools` ignores the function. In other words, using the `@operation` decorator on [controller](/controller) methods is required for this feature.
- `parameters`: a JSON Schema object describing `body`, `query`, and `params`, generated automatically by the [validation library](/validation).
- `execute`: a JavaScript function to be called when the LLM invokes the tool.

`createLLMTools` accepts an options object with:

- `modules`: a record of modules (explained below).
- `caller`: optional. A function that defines how an RPC method or controller‚Äôs callable method should run to produce `execute`. Intended for advanced scenarios.
- `onExecute`: optional. Called when `execute` completes successfully.
- `onError`: optional. Called when an `execute` call fails.
- `meta`: [metadata](/controller/meta) passed to each controller/RPC method. Can include any data you want available on the back end.
- `resultFormatter`: optional. Formats the result returned to the LLM. By default, returns the raw result. Use `"mcp"` to format according to [MCP](/mcp).

Each `modules` entry is either an RPC module or a controller. Controllers are functions with pre-populated features such as `schema` implementing `VovkHandlerSchema`, which includes the `operationObject` populated by the `@operation` decorator.

`createLLMTools` distinguishes RPC module methods from controller methods by the `isRPC` property. If `isRPC: true` is present, the method is treated as an RPC method. Otherwise, it is treated as a controller method that exposes `fn` (see [callable controller methods](/controller/fn)), mirroring the RPC function signature to allow direct invocation without HTTP.

```ts showLineNumbers copy
import { createLLMTools } from 'vovk';
import { PostRPC } from 'vovk-client';
import UserController from '../user/UserController';

const { tools } = createLLMTools({
  modules: {
    PostRPC,
    UserController,
  },
  onExecute: (tool, result) => {
    console.log(`Tool ${tool.name} executed successfully with result:`, result);
  },
  onError: (tool, error) => {
    console.error(`Tool ${tool.name} execution failed with error:`, error);
  },
});
```

In this example, `PostRPC` is an RPC module whose methods issue HTTP requests, while `UserController` is a controller exposing callable methods that can be used like regular functions outside of an HTTP request. This enables function calling on the server (for direct DB access) and on the client (via HTTP). The ‚Äúclient‚Äù can be a browser or any JavaScript environment that supports `fetch`, such as React Native, Node.js, or Edge Runtime.

## Selecting Specific Methods

To include only certain methods from a module, use the `pick`/`omit` pattern from `lodash` or a similar utility.

```ts showLineNumbers copy
import { createLLMTools } from 'vovk';
import { PostRPC } from 'vovk-client';
import { pick, omit } from 'lodash';
import UserController from '../user/UserController';

const { tools } = createLLMTools({
  modules: {
    PostRPC: pick(PostRPC, ['createPost', 'getPost']),
    UserController: omit(UserController, ['deleteUser']),
  },
});
```

The resulting `tools` include `createPost` and `getPost` from `PostRPC`, and all methods from `UserController` except `deleteUser`.

## Authorizing RPC Calls

To add authorization, pass a module as a tuple: the `module` itself and an `options` object with an `init` extending `RequestInit`. This lets you add headers such as auth tokens.

```ts showLineNumbers copy
import { createLLMTools } from 'vovk';
import { PostRPC } from 'vovk-client';

const { tools } = createLLMTools({
  modules: {
    PostRPC: [
      PostRPC,
      {
        init: {
          headers: {
            Authorization: `Bearer ${process.env.AUTH_TOKEN}`,
          },
        },
      },
    ],
  },
});
```

You can combine pick/omit with the `init` tuple to authorize and select methods at the same time:

```ts showLineNumbers copy
import { createLLMTools } from 'vovk';
import { PostRPC } from 'vovk-client';
import { pick } from 'lodash';

const { tools } = createLLMTools({
  modules: {
    PostRPCAuthorized: [
      pick(PostRPC, ['createPost']),
      {
        init: {
          headers: {
            Authorization: `Bearer ${process.env.AUTH_TOKEN}`,
          },
        },
      },
    ],
    PostRPCUnauthorized: pick(PostRPC, ['getPost']),
  },
});
```

<a id="x-tool" />
## Custom Operation Attributes

You can add custom attributes with the `x-tool-*` syntax to the OpenAPI operation in the `@operation` decorator, enabling AI-specific tooling metadata.

```ts showLineNumbers copy filename="src/modules/user/UserController.ts"
import { prefix, get, operation } from 'vovk';

@prefix('user')
export default class UserController {
  @operation({
    summary: 'Get user by ID',
    description: 'Retrieves a user by their unique ID.',
    'x-tool-disable': false,
    'x-tool-name': 'get_user_by_id',
    'x-tool-description':
      'Retrieves a user by their unique ID, including name and email. Also includes user roles and permissions that define what actions the user can perform within the system.',
    'x-tool-successMessage': 'User retrieved successfully.',
    'x-tool-errorMessage': 'Failed to retrieve user.',
    'x-tool-includeResponse': true,
  })
  @get('{id}')
  static getUser() {
    // ...
  }
}
```

### `x-tool-disable`

Force-disables the function for LLM function calling, even if `summary`, `description`, or `x-tool-description` is present.

### `x-tool-name`

Overrides the generated function name (normally in the form `${moduleName}_${handlerName}`).

### `x-tool-description`

Overrides the generated function description (normally derived from `summary` + `description`).

### `x-tool-successMessage`

[MCP](/mcp)-specific attribute sent to the LLM when the function executes successfully.

### `x-tool-errorMessage`

[MCP](/mcp)-specific attribute sent to the LLM when the function execution fails.

### `x-tool-includeResponse`

[MCP](/mcp)-specific attribute indicating whether the function response should be included in the message sent back to the LLM after successful execution. Defaults to `true`.

## Third-Party APIs

Vovk.ts can generate RPC modules from third-party OpenAPI specs, letting you combine your back-end with external APIs in a single agent. See the [Codegen guide](/codegen).

```ts showLineNumbers copy
import { createLLMTools } from 'vovk';
import { GithubIssuesAPI, TaskRPC } from 'vovk-client';
const { tools } = createLLMTools({
  modules: {
    GithubIssuesAPI, // 3rd-party API
    TaskRPC, // your own back-end API
  },
});
```

Because the Vovk.ts CLI can serve as a standalone code generator (e.g., for [NestJS](/nestjs)), you can still use the generated RPC modules with `createLLMTools`.

## Function Calling Example

You can implement function calling using raw AI APIs with [JSON Lines](/controller/jsonlines). In most cases, including this example, the [Vercel AI SDK](https://ai-sdk.dev/) provides a simple way to build a text chat with function calling.

### Create LLM Endpoint

Create a controller and pass your RPC modules or controllers to `createLLMTools` as shown above.

```sh npm2yarn copy
npx vovk new controller aiSdk --empty
```

Paste the following into `src/modules/ai-sdk/AiSdkController.ts`, adjusting imports as needed:

```ts showLineNumbers copy filename="src/modules/ai-sdk/AiSdkController.ts" {18}
import {
  createLLMTools,
  HttpException,
  HttpStatus,
  post,
  prefix,
  operation,
  type KnownAny,
  type VovkRequest,
} from 'vovk';
import {
  jsonSchema,
  streamText,
  tool,
  convertToModelMessages,
  type ModelMessage,
  type UIMessage,
  type JSONSchema7,
} from 'ai';
import { openai } from '@ai-sdk/openai';
import { PostRPC } from 'vovk-client';
import UserController from '@/modules/user/UserController';

@prefix('ai-sdk')
export default class AiSdkController {
  @operation({
    summary: 'Vercel AI SDK with Function Calling',
    description:
      'Uses [@ai-sdk/openai](https://www.npmjs.com/package/@ai-sdk/openai) and ai packages to call a function',
  })
  @post('function-calling')
  static async functionCalling(req: VovkRequest<{ messages: UIMessage[] }>) {
    const { messages } = await req.json();
    const { tools: llmTools } = createLLMTools({
      modules: { UserController, PostRPC },
      onExecute: (d) => console.log('Success', d),
      onError: (e) => console.error('Error', e),
    });

    const tools = Object.fromEntries(
      llmTools.map(({ name, execute, description, parameters }) => [
        name,
        tool({
          execute: async (args, { toolCallId }) => {
            return execute(args, { toolCallId });
          },
          description,
          inputSchema: jsonSchema(parameters as JSONSchema7),
        }),
      ])
    );

    return streamText({
      model: openai('gpt-5-nano'),
      system:
        'You are a helpful assistant. Always provide a clear confirmation message after executing any function. Explain what was done and what the results were after the user request is executed.',
      messages: convertToModelMessages(messages),
      tools,
      onError: (e) => console.error('streamText error', e),
      onFinish: ({ finishReason }) => {
        if (finishReason === 'tool-calls') {
          console.log('Tool calls finished');
        }
      },
    }).toUIMessageStreamResponse();
  }
}
```

Here, the `tools` are mapped to Vercel AI SDK `tool` instances using `jsonSchema`. For other Node.js libraries, you can map them differently.

### Create a Front-End Component

On the front end, create a component using the [useChat](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat) hook:

```ts showLineNumbers copy filename="src/app/page.tsx"
'use client';
import { useChat } from '@ai-sdk/react';
import { DefaultChatTransport } from 'ai';
import { useState } from 'react';

export default function Page() {
  const [input, setInput] = useState('');

  const { messages, sendMessage, error, status } = useChat({
    transport: new DefaultChatTransport({
      api: '/api/ai-sdk/function-calling',
    }),
  });

  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault();
    if (input.trim()) {
      sendMessage({ text: input });
      setInput('');
    }
  };

  return (
    <form onSubmit={handleSubmit}>
      {messages.map((message) => (
        <div key={message.id}>
          {message.role === 'assistant' ? 'ü§ñ' : 'üë§'}{' '}
          {message.parts.map((part, partIndex) => (
            <span key={partIndex}>{part.type === 'text' ? part.text : ''}</span>
          ))}
        </div>
      ))}
      {error && <div>‚ùå {error.message}</div>}
      <div className="input-group">
        <input type="text" placeholder="Send a message..." value={input} onChange={(e) => setInput(e.target.value)} />
        <button>Send</button>
      </div>
    </form>
  );
}
```

See the [Vercel AI SDK docs](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat) and the [LLM guide](/llm).

## Roadmap

- ‚ú® Add a `router` option to `createLLMTools` to support hundreds of functions without hitting LLM function-calling limits. Routing can be implemented using vector search or other approaches.
