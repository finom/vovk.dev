
# WebRTC-based Realtime Voice AI

For the JARVIS-like experience, mentioned at the [Overview](./overview) page, we're going to set up a voice interface that uses OpenAI Realtime API with WebRTC to send and receive audio data in real-time. This time, instead of controller methods that are executed by AI SDK on the back-end, we're going to use function calling capabilities to call the methods of the [RPC modules](/rpc-module) directly from the browser, making the experience truly real-time and almost instantaneous.

## Back-end Setup

For the back-end we're going to create a session endpoint that's implemented with the help of the [Realtime API with WebRTC article](https://platform.openai.com/docs/guides/realtime-webrtc) from the official OpenAI documentation. The endpoint is going to accept the SDP offer from the client, voice selection query parameter, and return the SDP answer from the OpenAI Realtime API.

```ts showLineNumbers copy filename="src/modules/realtime/RealtimeController.ts" repository="finom/realtime-kanban"
import { prefix, post, HttpException, HttpStatus } from "vovk";
import { z } from "zod";
import { withZod } from "@/lib/withZod";
import { sessionGuard } from "@/decorators/sessionGuard";

@prefix("realtime")
export default class RealtimeController {
  @post("session")
  @sessionGuard()
  static session = withZod({
    query: z.object({
      voice: z.enum(["ash", "ballad", "coral", "sage", "verse"]),
    }),
    body: z.object({ sdp: z.string() }),
    output: z.object({ sdp: z.string() }),
    async handle({ vovk }) {
      const voice = vovk.query().voice;
      const { sdp: sdpOffer } = (await vovk.body());
      const sessionConfig = JSON.stringify({
        type: "realtime",
        model: "gpt-realtime",
        audio: { output: { voice } },
      });

      const fd = new FormData();
      fd.set("sdp", sdpOffer);
      fd.set("session", sessionConfig);

      try {
        const r = await fetch("https://api.openai.com/v1/realtime/calls", {
          method: "POST",
          headers: {
            Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
          },
          body: fd,
        });
        // Send back the SDP we received from the OpenAI REST API
        const sdp = await r.text();
        return { sdp };
      } catch (error) {
        throw new HttpException(
          HttpStatus.INTERNAL_SERVER_ERROR,
          "Failed to generate token. " + String(error),
        );
      }
    },
  });
}
```
*[The code above is fetched from GitHub repository.](https://github.com/finom/realtime-kanban/blob/main/src/modules/realtime/RealtimeController.ts)*

## Front-end Setup

### Client-side Tools

For the front-end, let's create the client-side tools first. The tools array is going to include all the HTTP tools from the `TaskRPC` and `UserRPC` modules, as well as two custom tools: `getCurrentTime` and `partyMode`, borrowed from [this repository](https://github.com/cameronking4/openai-realtime-api-nextjs) to demonstrate custom tool creation. The list can be extended with other UI-related tools.

```ts showLineNumbers copy filename="src/lib/tools/index.ts" repository="finom/realtime-kanban" {7-9}
import { createTools, type VovkTool } from "vovk";
import { TaskRPC, UserRPC } from "vovk-client";
import getCurrentTime from "./getCurrentTime";
import partyMode from "./partyMode";

const tools: VovkTool[] = [
  ...createTools({
    modules: { TaskRPC, UserRPC },
  }).tools,
  {
    type: "function",
    name: "getCurrentTime",
    description: "Gets the current time in the user's timezone",
    parameters: {},
    execute: getCurrentTime,
  },
  {
    type: "function",
    name: "partyMode",
    description: "Triggers a confetti animation on the page",
    parameters: {},
    execute: partyMode,
  },
];

export default tools;
```
*[The code above is fetched from GitHub repository.](https://github.com/finom/realtime-kanban/blob/main/src/lib/tools/index.ts)*

### WebRTC Audio Session Hook

Next, we're going to create a custom hook `useWebRTCAudioSession` that manages the WebRTC session, including starting and stopping the session, handling audio streams, and managing the data channel for function calling.

The hook accepts the selected voice and the tools list as parameters. It returns the session state (`isActive`, `isTalking`) and a function to toggle the session (`toggleSession`).

The important parts of the hook are the `onopen` event handler of the data channel, where we send the `session.update` message with the tools list to inform the OpenAI Realtime API about the available tools, and the `onmessage` event handler, where we listen for function call requests from the model via `response.function_call_arguments.done` event, execute the corresponding tool, and send back the results.

```ts showLineNumbers copy filename="src/hooks/useWebRTCAudioSession.ts" repository="finom/realtime-kanban" {99-121,87-93}
"use client";
import { useState, useRef, useCallback, useEffect } from "react";
import { VovkTool } from "vovk";
import { RealtimeRPC } from "vovk-client";

/**
 * Hook to manage a real-time session with OpenAI's Realtime endpoints.
 * @example const { isActive, isTalking, handleStartStopClick } = useWebRTCAudioSession(voice, tools);
 */
export default function useWebRTCAudioSession(
  voice: "ash" | "ballad" | "coral" | "sage" | "verse",
  tools: VovkTool[],
) {
  const audioElement = useRef<HTMLAudioElement | null>(null);
  const [isActive, setIsActive] = useState(false);
  // Data channel ref
  const dcRef = useRef<RTCDataChannel | null>(null);
  // Media stream ref for microphone
  const mcRef = useRef<MediaStream | null>(null);
  // talking state + refs
  const [isTalking, setIsTalking] = useState(false);
  const remoteAnalyserRef = useRef<AnalyserNode | null>(null);
  const remoteMonitorIntervalRef = useRef<number | null>(null);
  const remoteAudioContextRef = useRef<AudioContext | null>(null);

  const startSession = useCallback(async () => {
    // Create a peer connection
    const pc = new RTCPeerConnection();

    // Set up to play remote audio from the model
    audioElement.current = document.createElement("audio");
    audioElement.current.autoplay = true;
    pc.ontrack = (e) => {
      audioElement.current!.srcObject = e.streams[0];
      // Simple audio activity monitor
      try {
        const audioCtx = new AudioContext();
        remoteAudioContextRef.current = audioCtx;
        const source = audioCtx.createMediaStreamSource(e.streams[0]);
        const analyser = audioCtx.createAnalyser();
        analyser.fftSize = 256;
        source.connect(analyser);
        remoteAnalyserRef.current = analyser;
        remoteMonitorIntervalRef.current = window.setInterval(() => {
          if (!remoteAnalyserRef.current) return;
          const a = remoteAnalyserRef.current;
          const data = new Uint8Array(a.fftSize);
          a.getByteTimeDomainData(data);
          let sum = 0;
          for (let i = 0; i < data.length; i++) {
            const v = (data[i] - 128) / 128;
            sum += v * v;
          }
          const rms = Math.sqrt(sum / data.length);
          setIsTalking(rms > 0.02); // simple threshold
        }, 200);
      } catch {
        // ignore audio activity errors
      }
    };

    // Add local audio track for microphone input in the browser
    const ms = await navigator.mediaDevices.getUserMedia({
      audio: true,
    });
    mcRef.current = ms;
    pc.addTrack(ms.getTracks()[0]);

    // Set up data channel for sending and receiving events
    const dc = pc.createDataChannel("oai-events");
    dcRef.current = dc;

    // Start the session using the Session Description Protocol (SDP)
    const offer = await pc.createOffer();
    await pc.setLocalDescription(offer);

    const { sdp } = await RealtimeRPC.session({
      body: { sdp: offer.sdp! },
      query: { voice },
    });

    await pc.setRemoteDescription({
      type: "answer",
      sdp,
    });
    dc.onopen = () => {
      const sessionUpdate = {
        type: "session.update",
        session: {
          type: "realtime",
          tools: tools.map(({ execute: _execute, ...toolRest }) => toolRest),
        },
      };
      dc.send(JSON.stringify(sessionUpdate));
    };
    dc.onmessage = async (event) => {
      const msg = JSON.parse(event.data);
      // Handle function call completions
      if (msg.type === "response.function_call_arguments.done") {
        const execute = tools.find((tool) => tool.name === msg.name)?.execute;
        if (execute) {
          const args = JSON.parse(msg.arguments);
          const result = await execute(args);

          // Respond with function output
          const response = {
            type: "conversation.item.create",
            item: {
              type: "function_call_output",
              call_id: msg.call_id,
              output: JSON.stringify(result),
            },
          };
          dcRef.current?.send(JSON.stringify(response));

          const responseCreate = {
            type: "response.create",
          };
          dcRef.current?.send(JSON.stringify(responseCreate));
        }
      }
    };
    setIsActive(true);
  }, []);

  const stopSession = useCallback(() => {
    // Close data channel and peer connection
    dcRef.current?.close();
    dcRef.current = null;
    // Stop microphone tracks
    mcRef.current?.getTracks().forEach((track) => track.stop());
    mcRef.current = null;
    // Close remote audio context
    remoteAudioContextRef.current?.close();
    remoteAudioContextRef.current = null;
    remoteAnalyserRef.current = null;
    // Stop the audio immediately
    if (audioElement.current) {
      audioElement.current.srcObject = null;
      audioElement.current = null;
    }
    // Clear monitoring interval
    if (remoteMonitorIntervalRef.current) {
      clearInterval(remoteMonitorIntervalRef.current);
      remoteMonitorIntervalRef.current = null;
    }
    setIsTalking(false);
    setIsActive(false);
  }, []);

  const toggleSession = useCallback(() => {
    if (isActive) {
      stopSession();
    } else {
      startSession();
    }
  }, [isActive, startSession, stopSession]);

  // Cleanup on unmount
  useEffect(() => {
    return () => stopSession();
  }, []);

  return {
    startSession,
    stopSession,
    toggleSession,
    isActive,
    isTalking,
  };
}
```
*[The code above is fetched from GitHub repository.](https://github.com/finom/realtime-kanban/blob/main/src/hooks/useWebRTCAudioSession.ts)*

Finally, we can create a simple component that uses the `useWebRTCAudioSession` hook and displays a floating button to start and stop the session, as well as indicate whether the model is currently talking.

```ts showLineNumbers copy filename="src/components/RealTimeDemo.tsx" repository="finom/realtime-kanban"
"use client";
import useWebRTCAudioSession from "@/hooks/useWebRTCAudioSession";
import tools from "@/lib/tools";
import Floaty from "./Floaty";

const RealTimeDemo = () => {
  const { isActive, isTalking, toggleSession } = useWebRTCAudioSession(
    "ash",
    tools,
  );

  return (
    <Floaty
      isActive={isActive}
      isTalking={isTalking}
      handleClick={toggleSession}
    />
  );
};

export default RealTimeDemo;
```
*[The code above is fetched from GitHub repository.](https://github.com/finom/realtime-kanban/blob/main/src/components/RealTimeDemo.tsx)*

The code for the `Floaty` component is not shown here for brevity, but you can find it [in the repository](https://github.com/finom/realtime-kanban/blob/main/src/components/Floaty.tsx).

With that, you now have a fully functional Realtime Voice AI interface that can interact with your application using natural language via voice, powered by OpenAI's Realtime API and Vovk.ts function calling capabilities!
